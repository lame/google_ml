{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Google ML Course Notebook\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "- [Framing](#Framing)\n",
    "- [Descending into ML](#Descending-into-ML)\n",
    "- [Reducing Loss](#Reducing-Loss)\n",
    "- [First Steps with Tensorflow](#First-Steps-with-Tensorflow)\n",
    "- [Generalization](#Generalization)\n",
    "- [Training and Test Sets](#Training-and-Test-Sets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Framing\n",
    "\n",
    "#### Terms\n",
    "\n",
    "**Label**: The true thing that we're predicting, the `y` variable in a linear regression\n",
    "\n",
    "**Features**: The input variable, the `x` variable in a linear regression. can be many `x₁, x₂, ..., xn`\n",
    "\n",
    "**Example**: 1 instance of data, such as **x** can be categorized as labeled or unlabeled. The boldface **x** means that this instance is a vector\n",
    "\n",
    "**Labeled Example**: `{features, label}: (x, y)` used to train the model\n",
    "\n",
    "\n",
    "| housingMedianAge(feature) | totalRooms(feature) | totalBedrooms(feature) | medianHouseValue(label) |\n",
    "|:------------------------- |:------------------- |:---------------------- |:----------------------- |\n",
    "| 15                        | 5612                | 1283                   | 66900                   |\n",
    "| 19                        | 7650                | 1901                   | 80100                   |\n",
    "| 17                        | 720                 | 174                    | 85700                   |\n",
    "| 14                        | 1501                | 337                    | 73400                   |\n",
    "| 20                        | 1454                | 326                    | 65500                   |\n",
    "\n",
    "**Unlabeled Example**: `{features, ?}: (x, ?)` used for making predictions on new data\n",
    "\n",
    "\n",
    "| housingMedianAge(feature) | totalRooms(feature) | totalBedrooms(feature) | \n",
    "|:-------------------------:|:-------------------:|:----------------------:|\n",
    "| 42                        | 1686                | 361                    |\n",
    "| 34                        | 1226                | 180                    |\n",
    "| 33                        | 1077                | 271                    |\n",
    "\n",
    "**Models**: maps examples to predicted label\n",
    "\n",
    "Models use **training** and **inference**\n",
    "\n",
    "Model **training** means creating or learning the model. Showing the model labeled examples and enabling the model to gradually learn the relationship\n",
    "\n",
    "Model **inference** means applying the trained model to unlebeled examples. In the example above the model couls predict `medianHouseValue`\n",
    "\n",
    "**Regression vs. Classification**\n",
    "\n",
    "A **regression** model predicts continuous values, think a linear regression or how much / how little\n",
    "\n",
    "A **classification** model predicts a discrete value, think a boolean value (spam/not spam) or what animal is this\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descending into ML\n",
    "\n",
    "**Linear Regression** is a best fit line\n",
    "\n",
    "$y' = w_1 x_1 + b$\n",
    " - $w_1$ is weight vector in one dimension\n",
    " - b stands for bias or Y Intercept, can also be referred to as $w_0$\n",
    " - $x_1$x is the feature in 1 dimension\n",
    " - $y'$ is the predicted label (desired output)\n",
    "\n",
    "### Loss\n",
    "\n",
    "**Loss** Distance of any given data point off of the linear regression line. Loss is on an absoulte scale\n",
    "\n",
    "**Classifying Loss**\n",
    "Squared Error, $L_2$ Loss: is calculated as the summation of the square of the difference between the model prediction and the true value\n",
    "\n",
    "$\\Sigma$: Summing over all the examples in the training set\n",
    "\n",
    "D: Average loss over all examples\n",
    "\n",
    "A more spohisticated model with three dimensions would be written as $y' = b + w_1x_1 + w_2x_2 + w_3x_3$\n",
    "\n",
    "### Training and Loss\n",
    "\n",
    "**Training** \n",
    "\n",
    "Training refers to the act of learning good values for all the weights and the bias from labeled examples.\n",
    "\n",
    "**Emperical Risk Minimization** supervised learning, build model to minimize loss\n",
    "\n",
    "The better a predictave model, the lover the overall average loss will be\n",
    "\n",
    "**Squared Loss ($L_2$ loss)** is the square of the differences between observation and preciction  \n",
    "= $(observation -prediction(x))^2$\n",
    "\n",
    "= $(y - y')^2$\n",
    "\n",
    "\n",
    "**Mean Square Error (MSE)** is the average squared loss per example over the whole dataset\n",
    "$MSE = \\frac{1}{N} \\sum_{(x,y)\\in D} (y - prediction(x))^2$\n",
    "\n",
    "$(x, y)$ is an example which $x$ is the set of features the model uses to make predictions and $y$ is the example's label\n",
    "\n",
    "$prediction(x)$ is a function of the weights and bias in combination with the set of features $x$\n",
    "\n",
    "$D$ is ta data set containing many labeled examples, which are made up of $(x, y)$ pairs\n",
    "\n",
    "$N$ is the number of examples in $D$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reducing Loss\n",
    "\n",
    "### An Iterative Approach\n",
    "\n",
    "The \"model\" takes one of more features as input and returns one prediction $(y')$ as output\n",
    "\n",
    "$b$ and $w_1$ can be arbitrary values, $(0, 0)$ is a simple place to start\n",
    "\n",
    "$y'$ is the model's prediction for $x$\n",
    "\n",
    "$y$ is the correct label for x, which loss is computed off of\n",
    "\n",
    "Iterate until overall loss stops changing or at least changes extremely slowly. When that happens, we say that the model has **converged**.\n",
    "\n",
    "### Gradient Descent\n",
    "\n",
    "Regression problems will always present a convex Loss vs. Weight trend for $w_1$. Because of this all regression problems have only 1 localized minimum where the slope is 0\n",
    "\n",
    "**Gradient of Loss** is the derivative (slope) of the curve at any given point $(x,y)$. Gradient is a vector of partial derivatives with respect to the weights.\n",
    "\n",
    "Gradient, because it's a vector, has both a direction and a magnitude.\n",
    "\n",
    "### Learning Rate\n",
    "\n",
    "Gradient descrne algorithms multiply the gradient by a scalar known as **learning rate** (step size) i.e.: Gradient maginture is 2.5 and the learning rate is 0.01, the next point will be 0.025 away. A perfect learning rate can be determined for one-dimension (feature) graphs is $\\frac{ 1 }{ f(x)'' }$. Higher dimension functions ideal learning rate can be determined with the inverse of the [Hessian Matrix](https://en.wikipedia.org/wiki/Hessian_matrix)\n",
    "\n",
    "### Stochastic Gradient Descent\n",
    "\n",
    "**Batch** is the total number of examples you use to calculate the gradient in a single iteration\n",
    "\n",
    "**Stochastic Gradient Descent (SGD)** Chooses examples at random, 1 example at a time (batch size of 1) per iteration. SGD will give optimal Gradient Descent given enough iterations but is very noisy\n",
    "\n",
    "**Mini-Batch Stochastic Gradient Descent (Mini-Batch SGD)** is a compromise between full-batch iteration and SGD. Sample sizes range between 10 and 1,000 chosen at random\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Steps with Tensorflow\n",
    "\n",
    "### Toolkit\n",
    "![Tensorflow Toolkit](assets/tensorflow_toolkit.png)\n",
    "\n",
    "Estimator is the High-level OOP API, tf.layers/tf.losses/tf.metrics are the libraries for common model components, and TensorFlow is the lower level APIs\n",
    "\n",
    "**tf.estimator API**\n",
    "Compatible with the scikit-learn API\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Intro to Pandas](first_steps_with_tensorflow/intro_to_pandas.ipynb)\n",
    "\n",
    "[First Steps with Tensorflow](first_steps_with_tensorflow/first_steps_with_tensorflow.ipynb)\n",
    "\n",
    "[Synthetic Features and Outliers](first_steps_with_tensorflow/synthetic_features_and_outliers.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalization\n",
    "\n",
    "### The Peril of Overfitting\n",
    "\n",
    "An **overfit model** gets a low loss during training but does a poor job predicting new data\n",
    "\n",
    "A good strategy to make sure that the model does not overvit the data is to split the training data set into a training set and a test set\n",
    "\n",
    "**Fine Print**\n",
    "- We draw examples independently and identically (i.i.d) at random from the distribution. In other words, examples don't influence each other. (An alternate explanation: i.i.d. is a way of referring to the randomness of variables.)\n",
    "- The distribution is stationary; that is the distribution doesn't change within the data set.\n",
    "- We draw examples from partitions from the same distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Test Sets\n",
    "\n",
    "The test set must be\n",
    "- Large enough to yield statistically meaningful results.\n",
    "- Representative of the data set as a whole. In other words, don't pick a test set with different characteristics than the training set.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

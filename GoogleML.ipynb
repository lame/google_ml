{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Google ML Course Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Framing\n",
    "\n",
    "#### Terms\n",
    "\n",
    "**Label**: The true thing that we're predicting, the `y` variable in a linear regression\n",
    "\n",
    "**Features**: The input variable, the `x` variable in a linear regression. can be many `x₁, x₂, ..., xn`\n",
    "\n",
    "**Example**: 1 instance of data, such as **x** can be categorized as labeled or unlabeled. The boldface **x** means that this instance is a vector\n",
    "\n",
    "**Labeled Example**: `{features, label}: (x, y)` used to train the model\n",
    "\n",
    "\n",
    "| housingMedianAge(feature) | totalRooms(feature) | totalBedrooms(feature) | medianHouseValue(label) |\n",
    "|:------------------------- |:------------------- |:---------------------- |:----------------------- |\n",
    "| 15                        | 5612                | 1283                   | 66900                   |\n",
    "| 19                        | 7650                | 1901                   | 80100                   |\n",
    "| 17                        | 720                 | 174                    | 85700                   |\n",
    "| 14                        | 1501                | 337                    | 73400                   |\n",
    "| 20                        | 1454                | 326                    | 65500                   |\n",
    "\n",
    "**Unlabeled Example**: `{features, ?}: (x, ?)` used for making predictions on new data\n",
    "\n",
    "\n",
    "| housingMedianAge(feature) | totalRooms(feature) | totalBedrooms(feature) | \n",
    "|:-------------------------:|:-------------------:|:----------------------:|\n",
    "| 42                        | 1686                | 361                    |\n",
    "| 34                        | 1226                | 180                    |\n",
    "| 33                        | 1077                | 271                    |\n",
    "\n",
    "**Models**: maps examples to predicted label\n",
    "\n",
    "Models use **training** and **inference**\n",
    "\n",
    "Model **training** means creating or learning the model. Showing the model labeled examples and enabling the model to gradually learn the relationship\n",
    "\n",
    "Model **inference** means applying the trained model to unlebeled examples. In the example above the model couls predict `medianHouseValue`\n",
    "\n",
    "**Regression vs. Classification**\n",
    "\n",
    "A **regression** model predicts continuous values, think a linear regression or how much / how little\n",
    "\n",
    "A **classification** model predicts a discrete value, think a boolean value (spam/not spam) or what animal is this\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descending into ML\n",
    "\n",
    "### Linear Regression\n",
    "\n",
    "**Linear Regression** is a best fit line\n",
    "\n",
    "$y' = w_1 x_1 + b$\n",
    " - $w_1$ is weight vector in one dimension\n",
    " - b stands for bias or Y Intercept, can also be referred to as $w_0$\n",
    " - $x_1$x is the feature in 1 dimension\n",
    " - $y'$ is the predicted label (desired output)\n",
    "\n",
    "**Loss** Distance of any given data point off of the linear regression line. Loss is on an absoulte scale\n",
    "\n",
    "**Classifying Loss**\n",
    "Squared Error, $L_2$ Loss: is calculated as the summation of the square of the difference between the model prediction and the true value\n",
    "\n",
    "$\\Sigma$: Summing over all the examples in the training set\n",
    "\n",
    "D: Average loss over all examples\n",
    "\n",
    "A more spohisticated model with three dimensions would be written as $y' = b + w_1x_1 + w_2x_2 + w_3x_3$\n",
    "\n",
    "### Training and Loss\n",
    "\n",
    "**Training** \n",
    "\n",
    "Training refers to the act of learning good values for all the weights and the bias from labeled examples.\n",
    "\n",
    "**Emperical Risk Minimization** supervised learning, build model to minimize loss\n",
    "\n",
    "The better a predictave model, the lover the overall average loss will be\n",
    "\n",
    "**Squared Loss ($L_2$ loss)** is the square of the differences between observation and preciction  \n",
    "= $(observation -prediction(x))^2$\n",
    "\n",
    "= $(y - y')^2$\n",
    "\n",
    "\n",
    "**Mean Square Error (MSE)** is the average squared loss per example over the whole dataset\n",
    "$MSE = \\frac{1}{N} \\sum_{(x,y)\\in D} (y - prediction(x))^2$\n",
    "\n",
    "$(x, y)$ is an example which $x$ is the set of features the model uses to make predictions and $y$ is the example's label\n",
    "\n",
    "$prediction(x)$ is a function of the weights and bias in combination with the set of features $x$\n",
    "\n",
    "$D$ is ta data set containing many labeled examples, which are made up of $(x, y)$ pairs\n",
    "\n",
    "$N$ is the number of examples in $D$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reducing Loss\n",
    "\n",
    "### An Iterative Approach\n",
    "\n",
    "The \"model\" takes one of more features as input and returns one prediction $(y')$ as output\n",
    "\n",
    "$b$ and $w_1$ can be arbitrary values, $(0, 0)$ is a simple place to start\n",
    "\n",
    "$y'$ is the model's prediction for $x$\n",
    "\n",
    "$y$ is the correct label for x, which loss is computed off of\n",
    "\n",
    "Iterate until overall loss stops changing or at least changes extremely slowly. When that happens, we say that the model has **converged**.\n",
    "\n",
    "### Gradient Descent\n",
    "\n",
    "Regression problems will always present a convex Loss vs. Weight trend for $w_1$. Because of this all regression problems have only 1 localized minimum where the slope is 0\n",
    "\n",
    "**Gradient of Loss** is the derivative (slope) of the curve at any given point $(x,y)$. Gradient is a vector of partial derivatives with respect to the weights.\n",
    "\n",
    "Gradient, because it's a vector, has both a direction and a magnitude.\n",
    "\n",
    "### Learning Rate\n",
    "\n",
    "Gradient descrne algorithms multiply the gradient by a scalar known as **learning rate** (step size) i.e.: Gradient maginture is 2.5 and the learning rate is 0.01, the next point will be 0.025 away. A perfect learning rate can be determined for one-dimension (feature) graphs is $\\frac{ 1 }{ f(x)'' }$. Higher dimension functions ideal learning rate can be determined with the inverse of the [Hessian Matrix](https://en.wikipedia.org/wiki/Hessian_matrix)\n",
    "\n",
    "### Stochastic Gradient Descent\n",
    "\n",
    "**Batch** is the total number of examples you use to calculate the gradient in a single iteration\n",
    "\n",
    "**Stochastic Gradient Descent (SGD)** Chooses examples at random, 1 example at a time (batch size of 1) per iteration. SGD will give optimal Gradient Descent given enough iterations but is very noisy\n",
    "\n",
    "**Mini-Batch Stochastic Gradient Descent (Mini-Batch SGD)** is a compromise between full-batch iteration and SGD. Sample sizes range between 10 and 1,000 chosen at random\n",
    "\n",
    "### Learning Rate and Convergence\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
